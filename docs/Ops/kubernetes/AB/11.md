# 11. Ingress

## Ingress Controller

### Requirement Resource fro Ingress Controller

- Deployment
- ConfigMap
- NodePort / LoadBalancer
- Auth
  - ServiceAccount
  - Role
  - ClusterRole
  - RoleBinding

[watch schematic resource requirements]

## Deploy an Ingress Controller

### Ingress Resources

- Routing
- SSL Configuration

## Configure Manager Tools

We have dozen configure manager tools for kubernetes.

But some of them is more popular, such as HashiCorp [Terraform], RedHat [Ansible] and Kubernetes native configure manager [Kustomize].

### HelmChart

#### Installation

```bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
```

update repository

```bash
helm repo update
```

check to added successfully

```bash
helm repo list
```

install chart

```bash
helm install my-ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace
```

check helm releases

```bash
helm list -A
```

## Troubleshooting

Layers of problems locations

1. Application Failure
2. ControlPlane Failure
3. WorkerNode Failure

### Application Failures

1. Watch the service status

```bash
curl http://web-service-ip:node-port
```

2. Describe the service and check it's parameters especially **Endpoints**, **Service** and **Pods**

```bash
kubectl describe svc <service-name>
```

3. Get resource list

```bash
kubectl get pods
```

> Pay attention to 'eady' and 'Status'

3.1. Get describe and logs of resources

```bash
kubectl describe pods <pod-name>
```

> Check the pod events

3.2. Get pod logs

```bash
kubectl logs <pod-name>
kubectl logs <pod-name> -f # follows pod logs
kubectl logs <pod-name> -f --previous # logs of previous container
```

4. Complete your debug section via **[debug-application]**

### Control Plane Failure

1. Check cluster nodes

```bash
kubectl get nodes
```

2. Get the list of system's pods

```bsh
kubectl -n kube-system get pods
```

**NOTE:** It is work if cluster set up via *Kubeadm*.

If not, you should check via systemd

```bash
systemctl status kube-apiserver
systemctl status kube-control-plane
systemctl status kube-schedular
```

Then get logs via **journalctl** or **kubectl logs**

3. Complete your debug section via **[debug-cluster]**

### Worker Node Failure

1. Check cluster nodes again

```bash
kubectl get nodes
```

2. Describe the worker nodes

```bash
kubectl describe nodes <worker-node-name>
```

2.1. If your worker node is on 'NOT RAEDY' status, check your 'Kubelet' first

2.2. Get more focus on 'Conditions' column

- Memory Pressure
- Disk Pressure
- PID Pressure

2.3. If your node status is 'UNKNOWN' that means the 'Kubelet' could not connect to 'API-Server'
> check the kubelet via ssh

**NOTE**: the 'Best Practice' for this purpose is use monitoring tools, like 'Prometheus' via 'Node Exporter'

3. Check the Kubelet service status and follow it's log

```bash
systemctl status kubelet
journalctl -u kubelet -f
```

4. Check the Kubelet Certificate

```bash
openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -text -noout
```

- **Issuer**: value must be 'KUBERNETES-CA'
- **Organization**: value must be 'system:nodes'

<!-- links -->
[Terraform]: https://developer.hashicorp.com/terraform/tutorials/kubernetes/kubernetes-provider
[Ansible]: https://docs.ansible.com/ansible/2.8/modules/kubernetes_module.html
[Kustomize]: https://kustomize.io/
[watch schematic resource requirements]: ../kubernetes_in_picture.md#ingres-controller-resources
[debug-application]: https://kubernetes.io/docs/tasks/debug/debug-application/
[debug-cluster]: https://kubernetes.io/docs/tasks/debug/debug-cluster/
